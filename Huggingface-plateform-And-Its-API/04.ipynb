{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fine Tuning Using Pre-traines Model:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine Tunning IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U accelerate\n",
    "# ! pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\DATA SCIENCE 101 DAYS CHALENGE\\abhi\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('4.46.2', '1.1.1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import accelerate\n",
    "import transformers\n",
    "\n",
    "transformers.__version__, accelerate.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "datasets=load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n"
     ]
    }
   ],
   "source": [
    "print(datasets['train']['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the Tokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the data \n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'],padding='max_length',truncation=True)\n",
    "\n",
    "tokenized_datasets=datasets.map(tokenize_function,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " 'label': 0,\n",
       " 'input_ids': [101,\n",
       "  1045,\n",
       "  12524,\n",
       "  1045,\n",
       "  2572,\n",
       "  8025,\n",
       "  1011,\n",
       "  3756,\n",
       "  2013,\n",
       "  2026,\n",
       "  2678,\n",
       "  3573,\n",
       "  2138,\n",
       "  1997,\n",
       "  2035,\n",
       "  1996,\n",
       "  6704,\n",
       "  2008,\n",
       "  5129,\n",
       "  2009,\n",
       "  2043,\n",
       "  2009,\n",
       "  2001,\n",
       "  2034,\n",
       "  2207,\n",
       "  1999,\n",
       "  3476,\n",
       "  1012,\n",
       "  1045,\n",
       "  2036,\n",
       "  2657,\n",
       "  2008,\n",
       "  2012,\n",
       "  2034,\n",
       "  2009,\n",
       "  2001,\n",
       "  8243,\n",
       "  2011,\n",
       "  1057,\n",
       "  1012,\n",
       "  1055,\n",
       "  1012,\n",
       "  8205,\n",
       "  2065,\n",
       "  2009,\n",
       "  2412,\n",
       "  2699,\n",
       "  2000,\n",
       "  4607,\n",
       "  2023,\n",
       "  2406,\n",
       "  1010,\n",
       "  3568,\n",
       "  2108,\n",
       "  1037,\n",
       "  5470,\n",
       "  1997,\n",
       "  3152,\n",
       "  2641,\n",
       "  1000,\n",
       "  6801,\n",
       "  1000,\n",
       "  1045,\n",
       "  2428,\n",
       "  2018,\n",
       "  2000,\n",
       "  2156,\n",
       "  2023,\n",
       "  2005,\n",
       "  2870,\n",
       "  1012,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1996,\n",
       "  5436,\n",
       "  2003,\n",
       "  8857,\n",
       "  2105,\n",
       "  1037,\n",
       "  2402,\n",
       "  4467,\n",
       "  3689,\n",
       "  3076,\n",
       "  2315,\n",
       "  14229,\n",
       "  2040,\n",
       "  4122,\n",
       "  2000,\n",
       "  4553,\n",
       "  2673,\n",
       "  2016,\n",
       "  2064,\n",
       "  2055,\n",
       "  2166,\n",
       "  1012,\n",
       "  1999,\n",
       "  3327,\n",
       "  2016,\n",
       "  4122,\n",
       "  2000,\n",
       "  3579,\n",
       "  2014,\n",
       "  3086,\n",
       "  2015,\n",
       "  2000,\n",
       "  2437,\n",
       "  2070,\n",
       "  4066,\n",
       "  1997,\n",
       "  4516,\n",
       "  2006,\n",
       "  2054,\n",
       "  1996,\n",
       "  2779,\n",
       "  25430,\n",
       "  14728,\n",
       "  2245,\n",
       "  2055,\n",
       "  3056,\n",
       "  2576,\n",
       "  3314,\n",
       "  2107,\n",
       "  2004,\n",
       "  1996,\n",
       "  5148,\n",
       "  2162,\n",
       "  1998,\n",
       "  2679,\n",
       "  3314,\n",
       "  1999,\n",
       "  1996,\n",
       "  2142,\n",
       "  2163,\n",
       "  1012,\n",
       "  1999,\n",
       "  2090,\n",
       "  4851,\n",
       "  8801,\n",
       "  1998,\n",
       "  6623,\n",
       "  7939,\n",
       "  4697,\n",
       "  3619,\n",
       "  1997,\n",
       "  8947,\n",
       "  2055,\n",
       "  2037,\n",
       "  10740,\n",
       "  2006,\n",
       "  4331,\n",
       "  1010,\n",
       "  2016,\n",
       "  2038,\n",
       "  3348,\n",
       "  2007,\n",
       "  2014,\n",
       "  3689,\n",
       "  3836,\n",
       "  1010,\n",
       "  19846,\n",
       "  1010,\n",
       "  1998,\n",
       "  2496,\n",
       "  2273,\n",
       "  1012,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  2054,\n",
       "  8563,\n",
       "  2033,\n",
       "  2055,\n",
       "  1045,\n",
       "  2572,\n",
       "  8025,\n",
       "  1011,\n",
       "  3756,\n",
       "  2003,\n",
       "  2008,\n",
       "  2871,\n",
       "  2086,\n",
       "  3283,\n",
       "  1010,\n",
       "  2023,\n",
       "  2001,\n",
       "  2641,\n",
       "  26932,\n",
       "  1012,\n",
       "  2428,\n",
       "  1010,\n",
       "  1996,\n",
       "  3348,\n",
       "  1998,\n",
       "  16371,\n",
       "  25469,\n",
       "  5019,\n",
       "  2024,\n",
       "  2261,\n",
       "  1998,\n",
       "  2521,\n",
       "  2090,\n",
       "  1010,\n",
       "  2130,\n",
       "  2059,\n",
       "  2009,\n",
       "  1005,\n",
       "  1055,\n",
       "  2025,\n",
       "  2915,\n",
       "  2066,\n",
       "  2070,\n",
       "  10036,\n",
       "  2135,\n",
       "  2081,\n",
       "  22555,\n",
       "  2080,\n",
       "  1012,\n",
       "  2096,\n",
       "  2026,\n",
       "  2406,\n",
       "  3549,\n",
       "  2568,\n",
       "  2424,\n",
       "  2009,\n",
       "  16880,\n",
       "  1010,\n",
       "  1999,\n",
       "  4507,\n",
       "  3348,\n",
       "  1998,\n",
       "  16371,\n",
       "  25469,\n",
       "  2024,\n",
       "  1037,\n",
       "  2350,\n",
       "  18785,\n",
       "  1999,\n",
       "  4467,\n",
       "  5988,\n",
       "  1012,\n",
       "  2130,\n",
       "  13749,\n",
       "  7849,\n",
       "  24544,\n",
       "  1010,\n",
       "  15835,\n",
       "  2037,\n",
       "  3437,\n",
       "  2000,\n",
       "  2204,\n",
       "  2214,\n",
       "  2879,\n",
       "  2198,\n",
       "  4811,\n",
       "  1010,\n",
       "  2018,\n",
       "  3348,\n",
       "  5019,\n",
       "  1999,\n",
       "  2010,\n",
       "  3152,\n",
       "  1012,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1045,\n",
       "  2079,\n",
       "  4012,\n",
       "  3549,\n",
       "  2094,\n",
       "  1996,\n",
       "  16587,\n",
       "  2005,\n",
       "  1996,\n",
       "  2755,\n",
       "  2008,\n",
       "  2151,\n",
       "  3348,\n",
       "  3491,\n",
       "  1999,\n",
       "  1996,\n",
       "  2143,\n",
       "  2003,\n",
       "  3491,\n",
       "  2005,\n",
       "  6018,\n",
       "  5682,\n",
       "  2738,\n",
       "  2084,\n",
       "  2074,\n",
       "  2000,\n",
       "  5213,\n",
       "  2111,\n",
       "  1998,\n",
       "  2191,\n",
       "  2769,\n",
       "  2000,\n",
       "  2022,\n",
       "  3491,\n",
       "  1999,\n",
       "  26932,\n",
       "  12370,\n",
       "  1999,\n",
       "  2637,\n",
       "  1012,\n",
       "  1045,\n",
       "  2572,\n",
       "  8025,\n",
       "  1011,\n",
       "  3756,\n",
       "  2003,\n",
       "  1037,\n",
       "  2204,\n",
       "  2143,\n",
       "  2005,\n",
       "  3087,\n",
       "  5782,\n",
       "  2000,\n",
       "  2817,\n",
       "  1996,\n",
       "  6240,\n",
       "  1998,\n",
       "  14629,\n",
       "  1006,\n",
       "  2053,\n",
       "  26136,\n",
       "  3832,\n",
       "  1007,\n",
       "  1997,\n",
       "  4467,\n",
       "  5988,\n",
       "  1012,\n",
       "  2021,\n",
       "  2428,\n",
       "  1010,\n",
       "  2023,\n",
       "  2143,\n",
       "  2987,\n",
       "  1005,\n",
       "  1056,\n",
       "  2031,\n",
       "  2172,\n",
       "  1997,\n",
       "  1037,\n",
       "  5436,\n",
       "  1012,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the training Arguments:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\DATA SCIENCE 101 DAYS CHALENGE\\abhi\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=0,\n",
       "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "average_tokens_across_devices=False,\n",
       "batch_eval_metrics=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_persistent_workers=False,\n",
       "dataloader_pin_memory=True,\n",
       "dataloader_prefetch_factor=None,\n",
       "ddp_backend=None,\n",
       "ddp_broadcast_buffers=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "dispatch_batches=None,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_do_concat_batches=True,\n",
       "eval_on_start=False,\n",
       "eval_steps=None,\n",
       "eval_strategy=IntervalStrategy.EPOCH,\n",
       "eval_use_gather_object=False,\n",
       "evaluation_strategy=None,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "gradient_checkpointing_kwargs=None,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_always_push=False,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=HubStrategy.EVERY_SAVE,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_for_metrics=[],\n",
       "include_inputs_for_metrics=False,\n",
       "include_num_input_tokens_seen=False,\n",
       "include_tokens_per_second=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=2e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=./results\\runs\\Nov12_23-35-14_MSI,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=500,\n",
       "logging_strategy=IntervalStrategy.STEPS,\n",
       "lr_scheduler_kwargs={},\n",
       "lr_scheduler_type=SchedulerType.LINEAR,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "neftune_noise_alpha=None,\n",
       "no_cuda=False,\n",
       "num_train_epochs=1,\n",
       "optim=OptimizerNames.ADAMW_TORCH,\n",
       "optim_args=None,\n",
       "optim_target_modules=None,\n",
       "output_dir=./results,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=16,\n",
       "per_device_train_batch_size=16,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=['tensorboard'],\n",
       "restore_callback_states_from_checkpoint=False,\n",
       "resume_from_checkpoint=None,\n",
       "run_name=./results,\n",
       "save_on_each_node=False,\n",
       "save_only_model=False,\n",
       "save_safetensors=True,\n",
       "save_steps=500,\n",
       "save_strategy=IntervalStrategy.STEPS,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "skip_memory_metrics=True,\n",
       "split_batches=None,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torch_empty_cache_steps=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_cpu=False,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_liger_kernel=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.01,\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args=TrainingArguments(\n",
    "    output_dir='./results', #Output Dir\n",
    "    eval_strategy='epoch', # Evaluate every epoch \n",
    "    learning_rate=2e-5, # learning rate\n",
    "    per_device_train_batch_size=16, # Batch size for training\n",
    "    per_device_eval_batch_size=16, # Batch size for evaluation\n",
    "    num_train_epochs=1, # Number of training epochs\n",
    "    weight_decay=0.01,  # Strength of weight decay \n",
    ")\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"'accelerate\"\n"
     ]
    }
   ],
   "source": [
    "pip install 'accelerate>={ACCELERATE_MIN_VERSION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification,Trainer\n",
    "\n",
    "# Load the Pre-trained model \n",
    "model=AutoModelForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer=Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'] \n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Fine Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./fine-tuned-model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./fine-tuned-model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save_pretrained('./fine-tuned-model')\n",
    "tokenizer.save_pretrained('./fine-tuned-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arxiv Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv\n",
      "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting feedparser~=6.0.10 (from arxiv)\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: requests~=2.32.0 in e:\\data science 101 days chalenge\\abhi\\lib\\site-packages (from arxiv) (2.32.3)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\data science 101 days chalenge\\abhi\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\data science 101 days chalenge\\abhi\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\data science 101 days chalenge\\abhi\\lib\\site-packages (from requests~=2.32.0->arxiv) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\data science 101 days chalenge\\abhi\\lib\\site-packages (from requests~=2.32.0->arxiv) (2024.2.2)\n",
      "Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "   ---------------------------------------- 0.0/81.3 kB ? eta -:--:--\n",
      "   ------------------------- -------------- 51.2/81.3 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 81.3/81.3 kB 1.5 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6060 sha256=aa957532f81c2a993b5b5b64a80901dbb5f577cc97898a102035f4375fdb3f89\n",
      "  Stored in directory: c:\\users\\abhi\\appdata\\local\\pip\\cache\\wheels\\03\\f5\\1a\\23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
      "Successfully installed arxiv-2.1.3 feedparser-6.0.11 sgmllib3k-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhi\\AppData\\Local\\Temp\\ipykernel_2696\\1274409406.py:7: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-11-11 18:59:02+00:00</td>\n",
       "      <td>UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts</td>\n",
       "      <td>The evaluation of mathematical reasoning capabilities is essential for\\nadvancing Artificial General Intelligence (AGI). While Large Language Models\\n(LLMs) have shown impressive performance in solving mathematical problems,\\nexisting benchmarks such as GSM8K and MATH present limitations, including\\nnarrow problem definitions with specific numbers and reliance on predetermined\\nrules that hinder accurate assessments of reasoning and adaptability. This\\npaper introduces the UTMath Benchmark, which robustly evaluates the models\\nthrough extensive unit tests. It consists of 1,053 problems across 9\\nmathematical domains, with over 68 test cases per problem.We propose an\\ninnovative evaluation framework inspired by unit testing in software\\ndevelopment, focusing on both accuracy and reliability of results. Furthermore,\\nwe introduce the Reasoning-to-Coding of Thoughts (RCoT) approach, which\\nencourages LLMs to perform explicit reasoning before generating code, leading\\nto generating more advanced solution and improved performance. Furthermore, we\\nare releasing not only the UTMath benchmark but also the UTMath-Train training\\ndataset (more than 70k samples), to support the community in further exploring\\nmathematical reasoning.</td>\n",
       "      <td>[cs.CL, cs.AI]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-11-11 18:58:46+00:00</td>\n",
       "      <td>DeepONet as a Multi-Operator Extrapolation Model: Distributed Pretraining with Physics-Informed Fine-Tuning</td>\n",
       "      <td>We propose a novel fine-tuning method to achieve multi-operator learning\\nthrough training a distributed neural operator with diverse function data and\\nthen zero-shot fine-tuning the neural network using physics-informed losses for\\ndownstream tasks. Operator learning effectively approximates solution operators\\nfor PDEs and various PDE-related problems, yet it often struggles to generalize\\nto new tasks. To address this, we investigate fine-tuning a pretrained model,\\nwhile carefully selecting an initialization that enables rapid adaptation to\\nnew tasks with minimal data. Our approach combines distributed learning to\\nintegrate data from various operators in pre-training, while physics-informed\\nmethods enable zero-shot fine-tuning, minimizing the reliance on downstream\\ndata. We investigate standard fine-tuning and Low-Rank Adaptation fine-tuning,\\napplying both to train complex nonlinear target operators that are difficult to\\nlearn only using random initialization. Through comprehensive numerical\\nexamples, we demonstrate the advantages of our approach, showcasing significant\\nimprovements in accuracy. Our findings provide a robust framework for advancing\\nmulti-operator learning and highlight the potential of transfer learning\\ntechniques in this domain.</td>\n",
       "      <td>[cs.LG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-11-11 18:54:30+00:00</td>\n",
       "      <td>Circulating Currents in Electric Machines: Positive Impact of The End Windings Length on Losses</td>\n",
       "      <td>Circulating currents occurring in windings of electric machines received\\nrising interest recent years. Circulating currents represent unwanted currents\\nflowing between parallel-connected conductors. This phenomenon is due to\\nvarious reasons such as asymmetries in the winding and differences in electric\\npotential between parallel-connected conductors. This effect occurs both at\\nno-load and on-load conditions, and always lead to uneven distribution of the\\ncurrent between the parallel conductors, therefore leading to higher losses, as\\nproven in the authors' previous work. Circulating currents are occurring mainly\\ndue to asymmetries and electric potential difference in the active part,\\nmeaning that long end windings are advantageous to mitigate the effect of\\ncirculating currents. Losses due to circulating currents decrease at a rate\\nproportional to the inverse square of the end windings length. The aim of this\\npaper is to mathematically prove this property and present a case study\\napplication in an electric machine.</td>\n",
       "      <td>[eess.SY, cs.SY, math-ph, math.MP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-11-11 18:51:08+00:00</td>\n",
       "      <td>Score-based generative diffusion with \"active\" correlated noise sources</td>\n",
       "      <td>Diffusion models exhibit robust generative properties by approximating the\\nunderlying distribution of a dataset and synthesizing data by sampling from the\\napproximated distribution. In this work, we explore how the generative\\nperformance may be be modulated if noise sources with temporal correlations --\\nakin to those used in the field of active matter -- are used for the\\ndestruction of the data in the forward process. Our numerical and analytical\\nexperiments suggest that the corresponding reverse process may exhibit improved\\ngenerative properties.</td>\n",
       "      <td>[cs.LG, cond-mat.dis-nn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-11-11 18:50:09+00:00</td>\n",
       "      <td>Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models</td>\n",
       "      <td>Adding Object into images based on text instructions is a challenging task in\\nsemantic image editing, requiring a balance between preserving the original\\nscene and seamlessly integrating the new object in a fitting location. Despite\\nextensive efforts, existing models often struggle with this balance,\\nparticularly with finding a natural location for adding an object in complex\\nscenes. We introduce Add-it, a training-free approach that extends diffusion\\nmodels' attention mechanisms to incorporate information from three key sources:\\nthe scene image, the text prompt, and the generated image itself. Our weighted\\nextended-attention mechanism maintains structural consistency and fine details\\nwhile ensuring natural object placement. Without task-specific fine-tuning,\\nAdd-it achieves state-of-the-art results on both real and generated image\\ninsertion benchmarks, including our newly constructed \"Additing Affordance\\nBenchmark\" for evaluating object placement plausibility, outperforming\\nsupervised methods. Human evaluations show that Add-it is preferred in over 80%\\nof cases, and it also demonstrates improvements in various automated metrics.</td>\n",
       "      <td>[cs.CV, cs.AI, cs.GR, cs.LG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-11-11 18:49:58+00:00</td>\n",
       "      <td>Watermark Anything with Localized Messages</td>\n",
       "      <td>Image watermarking methods are not tailored to handle small watermarked\\nareas. This restricts applications in real-world scenarios where parts of the\\nimage may come from different sources or have been edited. We introduce a\\ndeep-learning model for localized image watermarking, dubbed the Watermark\\nAnything Model (WAM). The WAM embedder imperceptibly modifies the input image,\\nwhile the extractor segments the received image into watermarked and\\nnon-watermarked areas and recovers one or several hidden messages from the\\nareas found to be watermarked. The models are jointly trained at low resolution\\nand without perceptual constraints, then post-trained for imperceptibility and\\nmultiple watermarks. Experiments show that WAM is competitive with state-of-the\\nart methods in terms of imperceptibility and robustness, especially against\\ninpainting and splicing, even on high-resolution images. Moreover, it offers\\nnew capabilities: WAM can locate watermarked areas in spliced images and\\nextract distinct 32-bit messages with less than 1 bit error from multiple small\\nregions - no larger than 10% of the image surface - even for small $256\\times\\n256$ images.</td>\n",
       "      <td>[cs.CV, cs.CR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-11-11 18:48:31+00:00</td>\n",
       "      <td>Learning from Limited and Imperfect Data</td>\n",
       "      <td>The datasets used for Deep Neural Network training (e.g., ImageNet, MSCOCO,\\netc.) are often manually balanced across categories (classes) to facilitate\\nlearning of all the categories. This curation process is often expensive and\\nrequires throwing away precious annotated data to balance the frequency across\\nclasses. This is because the distribution of data in the world (e.g., internet,\\netc.) significantly differs from the well-curated datasets and is often\\nover-populated with samples from common categories. The algorithms designed for\\nwell-curated datasets perform suboptimally when used to learn from imperfect\\ndatasets with long-tailed imbalances and distribution shifts. For deep models\\nto be widely used, getting away with the costly curation process by developing\\nrobust algorithms that can learn from real-world data distribution is\\nnecessary. Toward this goal, we develop practical algorithms for Deep Neural\\nNetworks that can learn from limited and imperfect data present in the real\\nworld. These works are divided into four segments, each covering a scenario of\\nlearning from limited or imperfect data. The first part of the works focuses on\\nLearning Generative Models for Long-Tail Data, where we mitigate the\\nmode-collapse for tail (minority) classes and enable diverse aesthetic image\\ngenerations as head (majority) classes. In the second part, we enable effective\\ngeneralization on tail classes through Inductive Regularization schemes, which\\nallow tail classes to generalize as the head classes without enforcing explicit\\ngeneration of images. In the third part, we develop algorithms for Optimizing\\nRelevant Metrics compared to the average accuracy for learning from long-tailed\\ndata with limited annotation (semi-supervised), followed by the fourth part,\\nwhich focuses on the effective domain adaptation of the model to various\\ndomains with zero to very few labeled samples.</td>\n",
       "      <td>[cs.CV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-11-11 18:46:37+00:00</td>\n",
       "      <td>Tooling or Not Tooling? The Impact of Tools on Language Agents for Chemistry Problem Solving</td>\n",
       "      <td>To enhance large language models (LLMs) for chemistry problem solving,\\nseveral LLM-based agents augmented with tools have been proposed, such as\\nChemCrow and Coscientist. However, their evaluations are narrow in scope,\\nleaving a large gap in understanding the benefits of tools across diverse\\nchemistry tasks. To bridge this gap, we develop ChemAgent, an enhanced\\nchemistry agent over ChemCrow, and conduct a comprehensive evaluation of its\\nperformance on both specialized chemistry tasks and general chemistry\\nquestions. Surprisingly, ChemAgent does not consistently outperform its base\\nLLMs without tools. Our error analysis with a chemistry expert suggests that:\\nFor specialized chemistry tasks, such as synthesis prediction, we should\\naugment agents with specialized tools; however, for general chemistry questions\\nlike those in exams, agents' ability to reason correctly with chemistry\\nknowledge matters more, and tool augmentation does not always help.</td>\n",
       "      <td>[cs.AI, cs.CE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-11-11 18:44:17+00:00</td>\n",
       "      <td>TempCharBERT: Keystroke Dynamics for Continuous Access Control Based on Pre-trained Language Models</td>\n",
       "      <td>With the widespread of digital environments, reliable authentication and\\ncontinuous access control has become crucial. It can minimize cyber attacks and\\nprevent frauds, specially those associated with identity theft. A particular\\ninterest lies on keystroke dynamics (KD), which refers to the task of\\nrecognizing individuals' identity based on their unique typing style. In this\\nwork, we propose the use of pre-trained language models (PLMs) to recognize\\nsuch patterns. Although PLMs have shown high performance on multiple NLP\\nbenchmarks, the use of these models on specific tasks requires customization.\\nBERT and RoBERTa, for instance, rely on subword tokenization, and they cannot\\nbe directly applied to KD, which requires temporal-character information to\\nrecognize users. Recent character-aware PLMs are able to process both subwords\\nand character-level information and can be an alternative solution.\\nNotwithstanding, they are still not suitable to be directly fine-tuned for KD\\nas they are not optimized to account for user's temporal typing information\\n(e.g., hold time and flight time). To overcome this limitation, we propose\\nTempCharBERT, an architecture that incorporates temporal-character information\\nin the embedding layer of CharBERT. This allows modeling keystroke dynamics for\\nthe purpose of user identification and authentication. Our results show a\\nsignificant improvement with this customization. We also showed the feasibility\\nof training TempCharBERT on a federated learning settings in order to foster\\ndata privacy.</td>\n",
       "      <td>[cs.CR, cs.CL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024-11-11 18:43:44+00:00</td>\n",
       "      <td>Grounding Video Models to Actions through Goal Conditioned Exploration</td>\n",
       "      <td>Large video models, pretrained on massive amounts of Internet video, provide\\na rich source of physical knowledge about the dynamics and motions of objects\\nand tasks. However, video models are not grounded in the embodiment of an\\nagent, and do not describe how to actuate the world to reach the visual states\\ndepicted in a video. To tackle this problem, current methods use a separate\\nvision-based inverse dynamic model trained on embodiment-specific data to map\\nimage states to actions. Gathering data to train such a model is often\\nexpensive and challenging, and this model is limited to visual settings similar\\nto the ones in which data are available. In this paper, we investigate how to\\ndirectly ground video models to continuous actions through self-exploration in\\nthe embodied environment -- using generated video states as visual goals for\\nexploration. We propose a framework that uses trajectory level action\\ngeneration in combination with video guidance to enable an agent to solve\\ncomplex tasks without any external supervision, e.g., rewards, action labels,\\nor segmentation masks. We validate the proposed approach on 8 tasks in Libero,\\n6 tasks in MetaWorld, 4 tasks in Calvin, and 12 tasks in iThor Visual\\nNavigation. We show how our approach is on par with or even surpasses multiple\\nbehavior cloning baselines trained on expert demonstrations while without\\nrequiring any action annotations.</td>\n",
       "      <td>[cs.RO, cs.AI, cs.CV, cs.LG]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  published  \\\n",
       "0 2024-11-11 18:59:02+00:00   \n",
       "1 2024-11-11 18:58:46+00:00   \n",
       "2 2024-11-11 18:54:30+00:00   \n",
       "3 2024-11-11 18:51:08+00:00   \n",
       "4 2024-11-11 18:50:09+00:00   \n",
       "5 2024-11-11 18:49:58+00:00   \n",
       "6 2024-11-11 18:48:31+00:00   \n",
       "7 2024-11-11 18:46:37+00:00   \n",
       "8 2024-11-11 18:44:17+00:00   \n",
       "9 2024-11-11 18:43:44+00:00   \n",
       "\n",
       "                                                                                                         title  \\\n",
       "0                                      UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts   \n",
       "1  DeepONet as a Multi-Operator Extrapolation Model: Distributed Pretraining with Physics-Informed Fine-Tuning   \n",
       "2              Circulating Currents in Electric Machines: Positive Impact of The End Windings Length on Losses   \n",
       "3                                      Score-based generative diffusion with \"active\" correlated noise sources   \n",
       "4                            Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models   \n",
       "5                                                                   Watermark Anything with Localized Messages   \n",
       "6                                                                     Learning from Limited and Imperfect Data   \n",
       "7                 Tooling or Not Tooling? The Impact of Tools on Language Agents for Chemistry Problem Solving   \n",
       "8          TempCharBERT: Keystroke Dynamics for Continuous Access Control Based on Pre-trained Language Models   \n",
       "9                                       Grounding Video Models to Actions through Goal Conditioned Exploration   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          abstract  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       The evaluation of mathematical reasoning capabilities is essential for\\nadvancing Artificial General Intelligence (AGI). While Large Language Models\\n(LLMs) have shown impressive performance in solving mathematical problems,\\nexisting benchmarks such as GSM8K and MATH present limitations, including\\nnarrow problem definitions with specific numbers and reliance on predetermined\\nrules that hinder accurate assessments of reasoning and adaptability. This\\npaper introduces the UTMath Benchmark, which robustly evaluates the models\\nthrough extensive unit tests. It consists of 1,053 problems across 9\\nmathematical domains, with over 68 test cases per problem.We propose an\\ninnovative evaluation framework inspired by unit testing in software\\ndevelopment, focusing on both accuracy and reliability of results. Furthermore,\\nwe introduce the Reasoning-to-Coding of Thoughts (RCoT) approach, which\\nencourages LLMs to perform explicit reasoning before generating code, leading\\nto generating more advanced solution and improved performance. Furthermore, we\\nare releasing not only the UTMath benchmark but also the UTMath-Train training\\ndataset (more than 70k samples), to support the community in further exploring\\nmathematical reasoning.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                We propose a novel fine-tuning method to achieve multi-operator learning\\nthrough training a distributed neural operator with diverse function data and\\nthen zero-shot fine-tuning the neural network using physics-informed losses for\\ndownstream tasks. Operator learning effectively approximates solution operators\\nfor PDEs and various PDE-related problems, yet it often struggles to generalize\\nto new tasks. To address this, we investigate fine-tuning a pretrained model,\\nwhile carefully selecting an initialization that enables rapid adaptation to\\nnew tasks with minimal data. Our approach combines distributed learning to\\nintegrate data from various operators in pre-training, while physics-informed\\nmethods enable zero-shot fine-tuning, minimizing the reliance on downstream\\ndata. We investigate standard fine-tuning and Low-Rank Adaptation fine-tuning,\\napplying both to train complex nonlinear target operators that are difficult to\\nlearn only using random initialization. Through comprehensive numerical\\nexamples, we demonstrate the advantages of our approach, showcasing significant\\nimprovements in accuracy. Our findings provide a robust framework for advancing\\nmulti-operator learning and highlight the potential of transfer learning\\ntechniques in this domain.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Circulating currents occurring in windings of electric machines received\\nrising interest recent years. Circulating currents represent unwanted currents\\nflowing between parallel-connected conductors. This phenomenon is due to\\nvarious reasons such as asymmetries in the winding and differences in electric\\npotential between parallel-connected conductors. This effect occurs both at\\nno-load and on-load conditions, and always lead to uneven distribution of the\\ncurrent between the parallel conductors, therefore leading to higher losses, as\\nproven in the authors' previous work. Circulating currents are occurring mainly\\ndue to asymmetries and electric potential difference in the active part,\\nmeaning that long end windings are advantageous to mitigate the effect of\\ncirculating currents. Losses due to circulating currents decrease at a rate\\nproportional to the inverse square of the end windings length. The aim of this\\npaper is to mathematically prove this property and present a case study\\napplication in an electric machine.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Diffusion models exhibit robust generative properties by approximating the\\nunderlying distribution of a dataset and synthesizing data by sampling from the\\napproximated distribution. In this work, we explore how the generative\\nperformance may be be modulated if noise sources with temporal correlations --\\nakin to those used in the field of active matter -- are used for the\\ndestruction of the data in the forward process. Our numerical and analytical\\nexperiments suggest that the corresponding reverse process may exhibit improved\\ngenerative properties.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Adding Object into images based on text instructions is a challenging task in\\nsemantic image editing, requiring a balance between preserving the original\\nscene and seamlessly integrating the new object in a fitting location. Despite\\nextensive efforts, existing models often struggle with this balance,\\nparticularly with finding a natural location for adding an object in complex\\nscenes. We introduce Add-it, a training-free approach that extends diffusion\\nmodels' attention mechanisms to incorporate information from three key sources:\\nthe scene image, the text prompt, and the generated image itself. Our weighted\\nextended-attention mechanism maintains structural consistency and fine details\\nwhile ensuring natural object placement. Without task-specific fine-tuning,\\nAdd-it achieves state-of-the-art results on both real and generated image\\ninsertion benchmarks, including our newly constructed \"Additing Affordance\\nBenchmark\" for evaluating object placement plausibility, outperforming\\nsupervised methods. Human evaluations show that Add-it is preferred in over 80%\\nof cases, and it also demonstrates improvements in various automated metrics.   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Image watermarking methods are not tailored to handle small watermarked\\nareas. This restricts applications in real-world scenarios where parts of the\\nimage may come from different sources or have been edited. We introduce a\\ndeep-learning model for localized image watermarking, dubbed the Watermark\\nAnything Model (WAM). The WAM embedder imperceptibly modifies the input image,\\nwhile the extractor segments the received image into watermarked and\\nnon-watermarked areas and recovers one or several hidden messages from the\\nareas found to be watermarked. The models are jointly trained at low resolution\\nand without perceptual constraints, then post-trained for imperceptibility and\\nmultiple watermarks. Experiments show that WAM is competitive with state-of-the\\nart methods in terms of imperceptibility and robustness, especially against\\ninpainting and splicing, even on high-resolution images. Moreover, it offers\\nnew capabilities: WAM can locate watermarked areas in spliced images and\\nextract distinct 32-bit messages with less than 1 bit error from multiple small\\nregions - no larger than 10% of the image surface - even for small $256\\times\\n256$ images.   \n",
       "6  The datasets used for Deep Neural Network training (e.g., ImageNet, MSCOCO,\\netc.) are often manually balanced across categories (classes) to facilitate\\nlearning of all the categories. This curation process is often expensive and\\nrequires throwing away precious annotated data to balance the frequency across\\nclasses. This is because the distribution of data in the world (e.g., internet,\\netc.) significantly differs from the well-curated datasets and is often\\nover-populated with samples from common categories. The algorithms designed for\\nwell-curated datasets perform suboptimally when used to learn from imperfect\\ndatasets with long-tailed imbalances and distribution shifts. For deep models\\nto be widely used, getting away with the costly curation process by developing\\nrobust algorithms that can learn from real-world data distribution is\\nnecessary. Toward this goal, we develop practical algorithms for Deep Neural\\nNetworks that can learn from limited and imperfect data present in the real\\nworld. These works are divided into four segments, each covering a scenario of\\nlearning from limited or imperfect data. The first part of the works focuses on\\nLearning Generative Models for Long-Tail Data, where we mitigate the\\nmode-collapse for tail (minority) classes and enable diverse aesthetic image\\ngenerations as head (majority) classes. In the second part, we enable effective\\ngeneralization on tail classes through Inductive Regularization schemes, which\\nallow tail classes to generalize as the head classes without enforcing explicit\\ngeneration of images. In the third part, we develop algorithms for Optimizing\\nRelevant Metrics compared to the average accuracy for learning from long-tailed\\ndata with limited annotation (semi-supervised), followed by the fourth part,\\nwhich focuses on the effective domain adaptation of the model to various\\ndomains with zero to very few labeled samples.   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       To enhance large language models (LLMs) for chemistry problem solving,\\nseveral LLM-based agents augmented with tools have been proposed, such as\\nChemCrow and Coscientist. However, their evaluations are narrow in scope,\\nleaving a large gap in understanding the benefits of tools across diverse\\nchemistry tasks. To bridge this gap, we develop ChemAgent, an enhanced\\nchemistry agent over ChemCrow, and conduct a comprehensive evaluation of its\\nperformance on both specialized chemistry tasks and general chemistry\\nquestions. Surprisingly, ChemAgent does not consistently outperform its base\\nLLMs without tools. Our error analysis with a chemistry expert suggests that:\\nFor specialized chemistry tasks, such as synthesis prediction, we should\\naugment agents with specialized tools; however, for general chemistry questions\\nlike those in exams, agents' ability to reason correctly with chemistry\\nknowledge matters more, and tool augmentation does not always help.   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                           With the widespread of digital environments, reliable authentication and\\ncontinuous access control has become crucial. It can minimize cyber attacks and\\nprevent frauds, specially those associated with identity theft. A particular\\ninterest lies on keystroke dynamics (KD), which refers to the task of\\nrecognizing individuals' identity based on their unique typing style. In this\\nwork, we propose the use of pre-trained language models (PLMs) to recognize\\nsuch patterns. Although PLMs have shown high performance on multiple NLP\\nbenchmarks, the use of these models on specific tasks requires customization.\\nBERT and RoBERTa, for instance, rely on subword tokenization, and they cannot\\nbe directly applied to KD, which requires temporal-character information to\\nrecognize users. Recent character-aware PLMs are able to process both subwords\\nand character-level information and can be an alternative solution.\\nNotwithstanding, they are still not suitable to be directly fine-tuned for KD\\nas they are not optimized to account for user's temporal typing information\\n(e.g., hold time and flight time). To overcome this limitation, we propose\\nTempCharBERT, an architecture that incorporates temporal-character information\\nin the embedding layer of CharBERT. This allows modeling keystroke dynamics for\\nthe purpose of user identification and authentication. Our results show a\\nsignificant improvement with this customization. We also showed the feasibility\\nof training TempCharBERT on a federated learning settings in order to foster\\ndata privacy.   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Large video models, pretrained on massive amounts of Internet video, provide\\na rich source of physical knowledge about the dynamics and motions of objects\\nand tasks. However, video models are not grounded in the embodiment of an\\nagent, and do not describe how to actuate the world to reach the visual states\\ndepicted in a video. To tackle this problem, current methods use a separate\\nvision-based inverse dynamic model trained on embodiment-specific data to map\\nimage states to actions. Gathering data to train such a model is often\\nexpensive and challenging, and this model is limited to visual settings similar\\nto the ones in which data are available. In this paper, we investigate how to\\ndirectly ground video models to continuous actions through self-exploration in\\nthe embodied environment -- using generated video states as visual goals for\\nexploration. We propose a framework that uses trajectory level action\\ngeneration in combination with video guidance to enable an agent to solve\\ncomplex tasks without any external supervision, e.g., rewards, action labels,\\nor segmentation masks. We validate the proposed approach on 8 tasks in Libero,\\n6 tasks in MetaWorld, 4 tasks in Calvin, and 12 tasks in iThor Visual\\nNavigation. We show how our approach is on par with or even surpasses multiple\\nbehavior cloning baselines trained on expert demonstrations while without\\nrequiring any action annotations.   \n",
       "\n",
       "                           categories  \n",
       "0                      [cs.CL, cs.AI]  \n",
       "1                             [cs.LG]  \n",
       "2  [eess.SY, cs.SY, math-ph, math.MP]  \n",
       "3            [cs.LG, cond-mat.dis-nn]  \n",
       "4        [cs.CV, cs.AI, cs.GR, cs.LG]  \n",
       "5                      [cs.CV, cs.CR]  \n",
       "6                             [cs.CV]  \n",
       "7                      [cs.AI, cs.CE]  \n",
       "8                      [cs.CR, cs.CL]  \n",
       "9        [cs.RO, cs.AI, cs.CV, cs.LG]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query to fetch AI-releted papers\n",
    "query='ai OP artificial intelligence OR machine learning'\n",
    "search=arxiv.Search(query=query,max_results=10,sort_by=arxiv.SortCriterion.SubmittedDate)\n",
    "\n",
    "# Fatch Papers\n",
    "papers=[]\n",
    "for result in search.results():\n",
    "    papers.append(\n",
    "        {\n",
    "            'published':result.published,\n",
    "            'title':result.title,\n",
    "            'abstract':result.summary,\n",
    "            'categories':result.categories,\n",
    "        }\n",
    "    )\n",
    "df=pd.DataFrame(papers)\n",
    "pd.set_option('display.max_colwidth',None)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example abstract from API\n",
    "abstract=df['abstract'][0]\n",
    "\n",
    "summarizer=pipeline('summarization',model='facebook/bart-large-cnn')\n",
    "\n",
    "### Summarization\n",
    "summarization_result=summarizer(abstract)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The evaluation of mathematical reasoning capabilities is essential for\\nadvancing Artificial General Intelligence (AGI). While Large Language Models\\n(LLMs) have shown impressive performance in solving mathematical problems,\\nexisting benchmarks such as GSM8K and MATH present limitations, including\\nnarrow problem definitions with specific numbers and reliance on predetermined\\nrules that hinder accurate assessments of reasoning and adaptability. This\\npaper introduces the UTMath Benchmark, which robustly evaluates the models\\nthrough extensive unit tests. It consists of 1,053 problems across 9\\nmathematical domains, with over 68 test cases per problem.We propose an\\ninnovative evaluation framework inspired by unit testing in software\\ndevelopment, focusing on both accuracy and reliability of results. Furthermore,\\nwe introduce the Reasoning-to-Coding of Thoughts (RCoT) approach, which\\nencourages LLMs to perform explicit reasoning before generating code, leading\\nto generating more advanced solution and improved performance. Furthermore, we\\nare releasing not only the UTMath benchmark but also the UTMath-Train training\\ndataset (more than 70k samples), to support the community in further exploring\\nmathematical reasoning.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['abstract'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The evaluation of mathematical reasoning capabilities is essential for advancing Artificial General Intelligence (AGI) Large Language Models have shown impressive performance in solving mathematical problems. Existing benchmarks such as GSM8K and MATH present limitations. This paper introduces the UTMath Benchmark, which robustly evaluates the models through extensive unit tests.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarization_result[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
